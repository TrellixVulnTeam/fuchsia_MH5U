// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// FIXME(allanmac): use a block expander here
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_EXT_control_flow_attributes : require

//
//
//

#ifndef NDEBUG
#extension GL_KHR_shader_subgroup_ballot : require
#endif

//
//
//

#include "spn_config.h"
#include "vk_layouts.h"

//
//
//

layout(local_size_x = SPN_DEVICE_PATHS_COPY_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_PATHS_COPY();

//
//
//

#define SPN_PATHS_COPY_SUBGROUP_SIZE (1 << SPN_DEVICE_PATHS_COPY_SUBGROUP_SIZE_LOG2)

#define SPN_PATHS_COPY_SUBGROUPS                                                                   \
  (SPN_DEVICE_PATHS_COPY_WORKGROUP_SIZE / SPN_PATHS_COPY_SUBGROUP_SIZE)

//
//
//

#define SPN_PATHS_COPY_SUBGROUP_MASK (SPN_PATHS_COPY_SUBGROUP_SIZE - 1)

#define SPN_PATHS_COPY_PATH_HEAD_DWORDS_FULL (SPN_PATH_HEAD_DWORDS & ~SPN_PATHS_COPY_SUBGROUP_MASK)

#define SPN_PATHS_COPY_PATH_HEAD_DWORDS_FRAC                                                       \
  (SPN_GLSL_MAX_MACRO(uint, SPN_PATH_HEAD_DWORDS_POW2_RU, SPN_PATHS_COPY_SUBGROUP_SIZE) &          \
   ~SPN_PATHS_COPY_SUBGROUP_MASK)

//
// clang-format off
//

#define SPN_PATHS_COPY_ONE_BITS  (SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_LOG2 + SPN_TAGGED_BLOCK_ID_BITS_TAG)
#define SPN_PATHS_COPY_ONE_MASK  SPN_GLSL_BITS_TO_MASK(SPN_PATHS_COPY_ONE_BITS)
#define SPN_PATHS_COPY_ONE       (1 << SPN_PATHS_COPY_ONE_BITS)

//
// clang-format on
//

#define SPN_PATHS_COPY_GET_ROLLING(diff)                                                           \
  SPN_BITFIELD_EXTRACT(uint(diff), SPN_PATHS_COPY_ONE_BITS, 32 - SPN_PATHS_COPY_ONE_BITS)

#define SPN_PATHS_COPY_UPDATE_ROLLING(tbid, bid)                                                   \
  (((bid) << SPN_TAGGED_BLOCK_ID_BITS_TAG) | ((tbid)&SPN_PATHS_COPY_ONE_MASK))

//
//
//

void
spn_copy_segs(const uint bp_block_idx, const uint pc_block_idx)
{
  [[unroll]]  //
  for (int ii = 0; ii < SPN_BLOCK_POOL_BLOCK_DWORDS; ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
  {
    bp_blocks[bp_block_idx + ii] = pc_ring[pc_block_idx + ii];
  }
}

//
//
//

void
spn_copy_node(const uint bp_block_idx, const uint pc_block_idx, const uint bp_base)
{
  //
  // DEBUG -- dump the head block
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_BLOCK_POOL_BLOCK_DWORDS);

    debug_base = subgroupBroadcast(debug_base, 0);

    for (int ii = 0; ii < SPN_BLOCK_POOL_BLOCK_DWORDS; ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
      {
        bp_debug[debug_base + ii + gl_SubgroupInvocationID] = pc_ring[pc_block_idx + ii];
      }
  }
#endif

  //
  // update host-initialized tagged block id "pointers" so that they
  // point to device-side blocks
  //
  for (int ii = 0; ii < SPN_BLOCK_POOL_BLOCK_DWORDS; ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
    {
      // load tagged_block_id word from host block
      uint tbid = pc_ring[pc_block_idx + ii];

      // calculate ahead of time
      const uint bp_offset = SPN_PATHS_COPY_GET_ROLLING(tbid - pc_rolling);
      const uint bp_id_idx = (bp_base + bp_offset) & bp_mask;

      // if tbid is not invalid then update its block id
      if (tbid != SPN_TAGGED_BLOCK_ID_INVALID)
        {
          const uint bid = bp_ids[bp_id_idx];

          tbid = SPN_PATHS_COPY_UPDATE_ROLLING(tbid, bid);
        }

      // store updated tagged block id to device-side block
      bp_blocks[bp_block_idx + ii] = tbid;
    }
}

//
//
//

void
spn_copy_head(const uint bp_block_idx,
              const uint pc_block_idx,
              const uint bp_base,
              const uint path_id_d)
{
  //
  // if the entire subgroup consists of path header words then copy
  // it... but also update the header map
  //
  // note that this loop bound might evaluate to zero
  //
  for (int ii = 0; ii < SPN_PATHS_COPY_PATH_HEAD_DWORDS_FULL; ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
    {
      const uint ph_dword = pc_ring[pc_block_idx + ii];

      bp_blocks[bp_block_idx + ii] = ph_dword;

      // the very first dword of the head is the host-side id
      if ((ii == 0) && (gl_SubgroupInvocationID == 0))
        bp_host_map[ph_dword] = path_id_d;
    }

  //
  // this case handles one subgroup that is a mix of header words and
  // tagged block ids
  //
  // if the loop starts at 0 then also update the host map
  //
  for (int ii = SPN_PATHS_COPY_PATH_HEAD_DWORDS_FULL; ii < SPN_PATHS_COPY_PATH_HEAD_DWORDS_FRAC;
       ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
    {
      uint tbid = pc_ring[pc_block_idx + ii];

      // the very first word of the head is the host-side id
#if (SPN_PATHS_COPY_PATH_HEAD_DWORDS_FULL == 0)
      if ((ii == 0) && (gl_SubgroupInvocationID == 0))
        {
          bp_host_map[tbid] = path_id_d;

#if 0
          const uint debug_base = atomicAdd(bp_debug_count[0], 2);
          bp_debug[debug_base + 0] = tbid;
          bp_debug[debug_base + 1] = path_id_d;
#endif
        }
#endif

      // calculate ahead of time
      const uint bp_offset = SPN_PATHS_COPY_GET_ROLLING(tbid - pc_rolling);
      const uint bp_id_idx = (bp_base + bp_offset) & bp_mask;

      // if this dword is not part of the path header and tbid is not
      // invalid then update its block id
      if ((ii + gl_SubgroupInvocationID) >= SPN_PATH_HEAD_DWORDS)
        {
          if (tbid != SPN_TAGGED_BLOCK_ID_INVALID)
            {
              const uint bid = bp_ids[bp_id_idx];

              tbid = SPN_PATHS_COPY_UPDATE_ROLLING(tbid, bid);
            }
        }

        //
        // DEBUG
        //
#if 0
        {
          uint debug_base = 0;

          if (gl_SubgroupInvocationID == 0)
            debug_base = atomicAdd(bp_debug_count[0], SPN_PATHS_COPY_SUBGROUP_SIZE);

          debug_base = subgroupBroadcast(debug_base, 0);

          bp_debug[debug_base + gl_SubgroupInvocationID] = tbid;
        }
#endif

      // store updated tagged block id to device-side block
      bp_blocks[bp_block_idx + ii] = tbid;
    }

  //
  // the remaining dwords are treated like a node
  //
  for (int ii = SPN_PATHS_COPY_PATH_HEAD_DWORDS_FRAC; ii < SPN_BLOCK_POOL_BLOCK_DWORDS;
       ii += SPN_PATHS_COPY_SUBGROUP_SIZE)
    {
      // load tagged_block_id dword from host block
      uint tbid = pc_ring[pc_block_idx + ii];

      // calculate ahead of time
      const uint bp_offset = SPN_PATHS_COPY_GET_ROLLING(tbid - pc_rolling);
      const uint bp_id_idx = (bp_base + bp_offset) & bp_mask;

      // if tbid is not invalid then update its block id
      if (tbid != SPN_TAGGED_BLOCK_ID_INVALID)
        {
          const uint bid = bp_ids[bp_id_idx];

          tbid = SPN_PATHS_COPY_UPDATE_ROLLING(tbid, bid);
        }

      // store updated tagged block id to device-side block
      bp_blocks[bp_block_idx + ii] = tbid;
    }
}

//
//
//

void
main()
{
  //
  // THERE ARE 3 TYPES OF PATH COPYING COMMANDS:
  //
  // - HEAD
  // - NODE
  // - SEGS
  //
  // THESE ARE SUBGROUP ORIENTED KERNELS
  //
  // A SUBGROUP CAN OPERATE ON [1,N] BLOCKS
  //

  //
  // It's likely that peak bandwidth is achievable with a single
  // workgroup.
  //
  // So let's keep the grids modestly sized and for simplicity and
  // portability let's also assume that a single workgroup can perform
  // all steps in the copy.
  //
  // Launch as large of a workgroup as possiblex
  //
  // 1. ATOMICALLY ALLOCATE BLOCKS BP_ELEMS POOL
  // 2. CONVERT COMMANDS IN PC_ELEMS BLOCK OFFSETS
  // 3. FOR EACH COMMAND:
  //      - HEAD: SAVE HEAD ID TO PC_ELEMS MAP.
  //              CONVERT AND COPY H INDICES.
  //
  //      - NODE: CONVERT AND COPY B INDICES
  //
  //      - SEGS: BULK COPY
  //
  // B : number of words in block -- always pow2
  // W : intelligently/arbitrarily chosen factor of B -- always pow2
  //

  // load the copied atomic read "base" from gmem
  SPN_SUBGROUP_UNIFORM const uint bp_base = pc_alloc[pc_alloc_idx];

  // every subgroup/simd that will work on the block loads the same command
#if (SPN_PATHS_COPY_SUBGROUPS == 1)

  SPN_SUBGROUP_UNIFORM
  const uint sid = gl_WorkGroupID.x;

#else

  SPN_SUBGROUP_UNIFORM
  const uint sid = gl_WorkGroupID.x * SPN_PATHS_COPY_SUBGROUPS + gl_SubgroupID;

  if (sid >= pc_span)
    return;  // this subgroup has no work

#endif

  // path builder data can be spread across two spans
  SPN_SUBGROUP_UNIFORM uint pc_idx = pc_head + sid;

  if (pc_idx >= pc_size)
    pc_idx -= pc_size;

  //
  // block and cmd rings share a buffer
  //
  // [<--- blocks --->|<--- cmds --->]
  //

  // broadcast load the command across the subgroup
  SPN_SUBGROUP_UNIFORM const uint pc_cmd = pc_ring[pc_size * SPN_BLOCK_POOL_BLOCK_DWORDS + pc_idx];

  // what do we want pc_elems do with this block?
  SPN_SUBGROUP_UNIFORM const uint tag = SPN_PATHS_COPY_CMD_GET_TYPE(pc_cmd);

  // compute offset from rolling base to get index into block pool ring allocation
  SPN_SUBGROUP_UNIFORM const uint bp_offset = SPN_PATHS_COPY_GET_ROLLING(pc_cmd - pc_rolling);

  // index into id ring
  SPN_SUBGROUP_UNIFORM const uint bp_ids_idx = (bp_base + bp_offset) & bp_mask;

  // convert the pc_cmd's block pool ring idx into a block id
  SPN_SUBGROUP_UNIFORM const uint path_id_d = bp_ids[bp_ids_idx];

  //
  // DEBUG -- dump the head block
  //
#if 0
  {
    if (gl_SubgroupInvocationID == 0)
      {
        uint debug_base = atomicAdd(bp_debug_count[0], 1);

        bp_debug[debug_base] = bp_ids_idx;
      }
  }
#endif

  // calculate bp_elems (to) and pc_elems (from) indices
  const uint bp_block_idx = path_id_d * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;
  const uint pc_block_idx = pc_idx * SPN_BLOCK_POOL_BLOCK_DWORDS + gl_SubgroupInvocationID;

  // switch on cmd type
  if (tag == SPN_PATHS_COPY_CMD_TYPE_SEGS)
    {
      spn_copy_segs(bp_block_idx, pc_block_idx);
    }
  else if (tag == SPN_PATHS_COPY_CMD_TYPE_NODE)
    {
      spn_copy_node(bp_block_idx, pc_block_idx, bp_base);
    }
  else  // (tag == SPN_PATHS_COPY_CMD_TYPE_HEAD)
    {
      spn_copy_head(bp_block_idx, pc_block_idx, bp_base, path_id_d);
    }
}

//
//
//
